{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs5d-jOeuvqZ"
      },
      "source": [
        "! pip install streamlit -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWNKZgLbSLsi"
      },
      "source": [
        "the cell python code into an app.py file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"dataset_a.zip\" -d \"\"\n",
        "!unzip \"dataset_bc.zip\" -d \"\""
      ],
      "metadata": {
        "id": "oELTnEe7Y6rs",
        "outputId": "5f6b729a-d675-48da-e533-7e20e3cc8d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset_a.zip\n",
            "  inflating: dataset_a.csv           \n",
            "Archive:  dataset_bc.zip\n",
            "  inflating: dataset_bc.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install xgboost"
      ],
      "metadata": {
        "id": "B7x3unWzefk5",
        "outputId": "02cced96-f823-43bb-ab7a-8628ff6bb776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gogGn3qEuxUt",
        "outputId": "4c184146-74ab-4758-9cf9-d1fac389dc8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "pd.set_option('display.float_format', '{:.0f}'.format)\n",
        "import pydeck as pdk\n",
        "import plotly.express as px\n",
        "\n",
        "st.set_option('deprecation.showPyplotGlobalUse', False)\n",
        "\n",
        "\n",
        "df_1 = pd.read_csv('dataset_a.csv')\n",
        "df_1.head()\n",
        "\n",
        "df_1['datetime'] = pd.to_datetime(df_1['datetime'],format='%Y%m%d%H%M%S')\n",
        "df_2 = pd.read_csv('dataset_bc.csv')\n",
        "df_2['datetime'] = pd.to_datetime(df_2['datetime'],format='%Y%m%d%H%M%S')\n",
        "\n",
        "df = pd.concat([df_1,df_2])\n",
        "\n",
        "\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "import seaborn as sns\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance\n",
        "#import joblib\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import *\n",
        "from sklearn.metrics import confusion_matrix,classification_report,roc_auc_score,accuracy_score,precision_score,recall_score,f1_score\n",
        "from sklearn.model_selection import train_test_split, BaseCrossValidator\n",
        "import time,datetime\n",
        "import sqlite3\n",
        "import pandas.io.sql as psql\n",
        "import csv,os,imp\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn import manifold, datasets\n",
        "from sklearn.preprocessing import normalize,Normalizer,StandardScaler,MinMaxScaler,scale \n",
        "train = df[(df['traintest']==0) & (df['abc']=='c')]  #a,c\n",
        "test = df[(df['traintest']==1) & (df['abc']=='a')] #a,c\n",
        "date_1 = train['datetime'] \n",
        "date_2 = test['datetime']\n",
        "y_train = train['labelnew'].values  \n",
        "\n",
        "train = train.drop(['labelnew','abc','traintest','datetime','node'\n",
        "                    ],axis=1)\n",
        "\n",
        "X_train =  train.values  \n",
        "\n",
        "y_test = test['labelnew'].values\n",
        "\n",
        "test = test.drop(['labelnew','abc','traintest','datetime','node'\n",
        "                    ],axis=1)\n",
        "X_test =  test.values  \n",
        "\n",
        "print(\"----------------------- Xgboost start ---------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "clf = XGBClassifier(\n",
        "    silent=0,  # \n",
        "    nthread = -1, # \n",
        "    tree_method= 'exact', #'approx', #'hist', #'approx', #approx,exact,hist\n",
        "    #max_bin=12, #240, #default=256\n",
        "    booster='gbtree',#dark  gbtree  \n",
        "    #booster='dark',#dark\n",
        "    #sample_type='weighted', #uniform\n",
        "    #normalize_type='forest', #tree\n",
        "    #rate_drop=0.9,   \n",
        "    n_estimators=80,  # \n",
        "    max_depth=4, #\n",
        "    learning_rate=0.2, #\n",
        "    #min_child_weight = 0.9, default=1\n",
        "    max_delta_step=1, #\n",
        "    #scale_pos_weight = 130, # 1778034/986# \n",
        "    #base_score=0.90, #\n",
        "    #reg_lambda=10, #default=1\n",
        "    #gamma=0,\n",
        "    subsample=0.85, #\n",
        "    #gamma = 0.1,# 0.1~0.2\n",
        "    #subsample=1, # \n",
        "    colsample_bytree=0.8, # \n",
        "    #reg_lambda=1, #\n",
        "    #reg_alpha=0, # L1\n",
        "    #max_delta_step=2,#1  # \n",
        "    #scale_pos_weight = 0.0017499ï¼Œ#sum(negative instances)/sum(positive instances)\n",
        "    #booster='gbtree',#dark\n",
        "    objective ='binary:logistic',#'binary:logistic', #'multi:softprob', #'multi:softmax' \n",
        "    #base_score=0.66,worse\n",
        "    #bjective ='multi:softprob',#'binary:logistic', #'multi:softprob', #'multi:softmax' \n",
        "    #um_class = 2,  # multisoftmax\n",
        "    #seed = 1440,  # \n",
        "    #missing=None,\n",
        "    #eval_metric ='auc',#error\n",
        "    )\n",
        "import joblib\n",
        "print('make model')\n",
        "clf.fit(X_train, y_train)\n",
        "joblib.dump(clf, 'Xgboost.pkl')\n",
        "\n",
        "print(\"Xgboost training cost time : \",time.time()-start_time)\n",
        "\n",
        "print('# -----------------6: Xgboost: train scoring ---------------')\n",
        "#load model\n",
        "rfcbuild = joblib.load('Xgboost.pkl')\n",
        "TrainResult = rfcbuild.predict(X_train)\n",
        "\n",
        "colNum = TrainResult.shape[0]\n",
        "TrainResult = TrainResult.reshape(colNum,1)\n",
        "\n",
        "print(\"confusion_matrix:\")\n",
        "print(confusion_matrix(y_train,TrainResult))\n",
        "print('---------')\n",
        "print(classification_report(y_train,TrainResult))\n",
        "roc_auc1 = roc_auc_score(y_train,TrainResult)\n",
        "print(\"Area under the ROC curve : %f\" % roc_auc1)\n",
        "\n",
        "print(\"------------------ Xgboost: test scoring ---------------\")\n",
        "\n",
        "TestResult = rfcbuild.predict(X_test)\n",
        "colNum = TestResult.shape[0]\n",
        "TestResult = TestResult.reshape(colNum,1)\n",
        "\n",
        "print(\"confusion_matrix:\")\n",
        "print(confusion_matrix(y_test,TestResult))\n",
        "print('---------')\n",
        "print(classification_report(y_test,TestResult))\n",
        "roc_auc1 = roc_auc_score(y_test,TestResult)\n",
        "print(\"Area under the ROC curve : %f\" % roc_auc1)\n",
        "\n",
        "# Feature Importance\n",
        "plt.figure(figsize=(10, 5))\n",
        "fea = rfcbuild.feature_importances_\n",
        "maxrange=np.max(fea)-np.min(fea)\n",
        "\n",
        "fea_normed = 100 * fea/np.max(fea)\n",
        "#fea_normed = fea/maxrange\n",
        "#fea_normed = (fea-np.min(fea))/maxrange\n",
        "#print('Importance score: ',type(fea_normed),fea_normed.shape,fea_normed)\n",
        "np.set_printoptions(suppress=True)\n",
        "print('Importance score: ')\n",
        "print(np.round(fea_normed,2))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(len(rfcbuild.feature_importances_)), fea_normed)\n",
        "plt.title(\"Xgboost: Feature Importance\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Normalized Importance Score\")\n",
        "plt.show()  \n",
        "\n",
        "print(' ---------  Output Test (predict) results to csv  -----')\n",
        "print('----------- Xgboost train and testsuccessful completed-----------------')\n",
        "\n",
        "x_train = pd.DataFrame(X_train,columns = train.columns)\n",
        "x_test = pd.DataFrame(X_test,columns = test.columns)\n",
        "importance = fea_normed.tolist()\n",
        "index = [i for i, e in enumerate(importance) if e >0]\n",
        "\n",
        "x_train_new = x_train.iloc[:,index]\n",
        "x_train_new = pd.concat([date_1.reset_index(drop=True),x_train_new,pd.Series(y_train)],axis=1)\n",
        "\n",
        "\n",
        "x_test_new = x_test.iloc[:,index]\n",
        "x_test_new = pd.concat([date_2.reset_index(drop=True),x_test_new,pd.Series(y_test)],axis=1)\n",
        "\n",
        "\n",
        "data = pd.concat([x_train_new,x_test_new])\n",
        "\n",
        "\n",
        "data = data.rename(columns = {0:'label'})\n",
        "\n",
        "data.isna().sum()\n",
        "data = data.fillna(method='bfill')\n",
        "\n",
        "\n",
        "df_sorted=data.sort_values(by='datetime')\n",
        "df_sorted = df_sorted.set_index(pd.DatetimeIndex(df_sorted['datetime']))\n",
        "df_sorted.drop(columns ='datetime',inplace = True)\n",
        "\n",
        "#120\n",
        "def plot_sorted():\n",
        "  for col in df_sorted.columns[:-1]:   \n",
        "   \n",
        "    plt.figure(figsize=(15,8))\n",
        "    plt.plot(df_sorted.index,df_sorted[col])\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(col)\n",
        "    plt.title(f'Date Vs {col}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "df_sorted = df_sorted.iloc[:,2:]\n",
        "\n",
        "def plot_box_plot():\n",
        "  plt.figure(figsize=[15,8])\n",
        "  sns.boxplot(data=df_sorted)\n",
        "  plt.xticks(rotation=90)\n",
        "\n",
        "x=plot_box_plot()  \n",
        "\n",
        "\n",
        "def time_series_visualization():\n",
        "  plt.style.use('fivethirtyeight')\n",
        "  df_sorted.plot(subplots=True,\n",
        "        layout=(6, 5),\n",
        "        figsize=(22,22),\n",
        "        fontsize=10, \n",
        "        linewidth=2,\n",
        "        sharex=False,\n",
        "        title='Visualization of the original Time Series')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#df_transform\n",
        "df_transform = df_sorted.loc[:,:'software-oper_cisco-platform-software_control-processes_control-process_per-core-stats_per-core-stat_idle.1'].copy().pct_change(1)\n",
        "\n",
        "# fill the 1st row with NA data\n",
        "df_transform.fillna(method='bfill', inplace=True)\n",
        "df_transform = df_transform.replace(np.inf,np.nan)\n",
        "# fill the 1st row with NA data\n",
        "df_transform.fillna(method='bfill', inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot the transformed time series\n",
        "def plot_transformed_feature():\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    df_transform.plot(subplots=True,\n",
        "                      layout=(6, 5),\n",
        "                      figsize=(24,24),\n",
        "                      fontsize=10, \n",
        "                      linewidth=2, \n",
        "                      title='Visualization of the transformed Features')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Split train and test data\n",
        "train_features = df_transform.loc['2021-01-28 17:44:40':'2021-02-08 17:44:40']\n",
        "train_labels = df_sorted.loc['2021-01-28 17:44:40':'2021-02-08 17:44:40', 'label']\n",
        "\n",
        "test_features = df_transform.loc['2021-02-08 17:44:40':'2021-02-10 13:37:40']\n",
        "test_labels = df_sorted.loc['2021-02-08 17:44:40':'2021-02-10 13:37:40', 'label']\n",
        "\n",
        "# I want to use a T-days window of input data for predicting target_class\n",
        "# It means I need to prepend (T-1) last train records to the 1st test window\n",
        "T = 45  # my choice of the timesteps window\n",
        "\n",
        "prepend_features = train_features.iloc[-(T-1):]\n",
        "test_features = pd.concat([prepend_features, test_features], axis=0)\n",
        "\n",
        "train_features.shape, train_labels.shape, test_features.shape, test_labels.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Rescale the features\n",
        "from sklearn.preprocessing import StandardScaler  # MinMaxScaler\n",
        "\n",
        "scaler = StandardScaler()  # MinMaxScaler(feature_range=(-1,1))\n",
        "scaled_train_features = pd.DataFrame(scaler.fit_transform(train_features.values),\n",
        "                                     index=train_features.index,\n",
        "                                     columns=train_features.columns)\n",
        "# The Scaler is fit on the training set and then applied to the test set\n",
        "scaled_test_features = pd.DataFrame(scaler.transform(test_features.values),\n",
        "                                    index=test_features.index,\n",
        "                                    columns=test_features.columns)\n",
        "\n",
        "scaled_train_features.shape, scaled_test_features.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot the rescaled_train_features\n",
        "def rescaled_train_features():\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    scaled_train_features.plot(subplots=True,\n",
        "                              layout=(6, 5),\n",
        "                              figsize=(24,24),\n",
        "                              fontsize=10, \n",
        "                              linewidth=2, \n",
        "                              title='Visualization of the scaled Train Features')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "st.title(\"EXplortory data analysis for LSTM \")\n",
        "menu = [\"Home\",\"EDA\",\"About\"]\n",
        "choice = st.sidebar.selectbox('Menu',menu)\n",
        "if choice == 'Home':\n",
        "    st.title(\"Choose From Menu ^_^ \")\n",
        "    st.title(\"<Exploratory Data Analysis for LSTM > Or <Home>\")\n",
        "elif choice == 'EDA':\n",
        "    st.subheader(\"Box PLot Visualization \")\n",
        "    box=plot_box_plot()  \n",
        "    st.pyplot(box)\t\n",
        "\n",
        "    st.subheader(\"Time Series Visualization From The Original Data \")\n",
        "    p_timeseries=time_series_visualization()\n",
        "    st.pyplot(p_timeseries)\n",
        "\n",
        "    st.subheader(\"Time Series Visualization From The Transformed Feature \")\n",
        "    plot_transformed=plot_transformed_feature()\n",
        "    st.pyplot(plot_transformed)\n",
        "\n",
        "    st.subheader(\"Time Series Visualization From The Rescaled Train Feature \")\n",
        "    rescaled_train=rescaled_train_features()\n",
        "    st.pyplot(rescaled_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "elif choice == 'About':\n",
        "    st.subheader(\"About page \")    \n",
        "else:\n",
        "    st.subheader(\"nothing page \") \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaUIzq8Nu4QZ",
        "outputId": "aa271f0b-c07e-4cb1-df96-a6313bdd7844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.7/dist-packages (5.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkz6oohKvd4R"
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"2FOkOpGt9VvDInSVYVgDmtyw7Vx_6zgpm4qtf1EMzgFUxqoVa\") #ngrok.com"
      ],
      "metadata": {
        "id": "By4KbSE657N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0saJbOCvaNq",
        "outputId": "37b32989-9a1b-4239-d798-83c26f1fca45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nohup streamlit run app.py --server.port 80 &"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = ngrok.connect(port = '80')\n",
        "print(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHUPsCXoPuml",
        "outputId": "10eda84b-f2b8-4678-c573-ffd26b15a498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"http://989c-35-227-108-163.ngrok.io\" -> \"http://localhost:80\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "tunnels = ngrok.get_tunnels()\n",
        "tunnels\n",
        "\n",
        "# try:\n",
        "#     # Block until CTRL-C or some other terminating event\n",
        "#     ngrok_process.proc.wait()\n",
        "# except KeyboardInterrupt:\n",
        "#     print(\" Shutting down server.\")\n",
        "\n",
        "#     ngrok.kill()"
      ],
      "metadata": {
        "id": "_DTLQgOa6H6B",
        "outputId": "c941a006-0d8c-4f53-ab15-6f9d9e255289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<NgrokTunnel: \"https://989c-35-227-108-163.ngrok.io\" -> \"http://localhost:80\">,\n",
              " <NgrokTunnel: \"http://989c-35-227-108-163.ngrok.io\" -> \"http://localhost:80\">]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnP63sVHxKT2"
      },
      "source": [
        "Run streamlit using `localtunnel` \n",
        "\n",
        "npm installation would take some time for the first time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "654JzyOJxRnJ"
      },
      "source": [
        "ngrok.kill()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bAHfgzBLOhfw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}